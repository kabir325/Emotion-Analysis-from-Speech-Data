{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa0a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kabir\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kabir\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kabir\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 5.0/11.0 MB 30.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 31.2 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\kabir\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb5085",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion Analysis from Speech Data - Unsupervised Learning\n",
    "# Capstone Project 2: Building and Comparing ML/DL Models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "import librosa\n",
    "import librosa.display\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA LOADING AND EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_audio_files(file_paths):\n",
    "    \"\"\"Load audio files and extract basic information\"\"\"\n",
    "    audio_data = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            audio_data.append({\n",
    "                'file_path': file_path,\n",
    "                'audio': y,\n",
    "                'sample_rate': sr,\n",
    "                'duration': len(y) / sr\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    return audio_data\n",
    "\n",
    "# Example: Load your audio files\n",
    "audio_files = load_audio_files(\"/Kaggle_Testset/\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. FEATURE EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def extract_audio_features(audio_data):\n",
    "    \"\"\"Extract comprehensive audio features for emotion analysis\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for audio_info in audio_data:\n",
    "        y = audio_info['audio']\n",
    "        sr = audio_info['sample_rate']\n",
    "        \n",
    "        # Basic audio features\n",
    "        feature_vector = {}\n",
    "        \n",
    "        # 1. Spectral Features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        feature_vector['spectral_centroid_mean'] = np.mean(spectral_centroids)\n",
    "        feature_vector['spectral_centroid_std'] = np.std(spectral_centroids)\n",
    "        \n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "        feature_vector['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "        feature_vector['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "        \n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "        feature_vector['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "        feature_vector['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "        \n",
    "        # 2. Zero Crossing Rate (relates to speech/music distinction)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "        feature_vector['zcr_mean'] = np.mean(zcr)\n",
    "        feature_vector['zcr_std'] = np.std(zcr)\n",
    "        \n",
    "        # 3. MFCCs (Mel-frequency cepstral coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        for i in range(13):\n",
    "            feature_vector[f'mfcc_{i}_mean'] = np.mean(mfccs[i])\n",
    "            feature_vector[f'mfcc_{i}_std'] = np.std(mfccs[i])\n",
    "        \n",
    "        # 4. Chroma Features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        feature_vector['chroma_mean'] = np.mean(chroma)\n",
    "        feature_vector['chroma_std'] = np.std(chroma)\n",
    "        \n",
    "        # 5. Mel Spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        feature_vector['mel_spectrogram_mean'] = np.mean(mel_spectrogram)\n",
    "        feature_vector['mel_spectrogram_std'] = np.std(mel_spectrogram)\n",
    "        \n",
    "        # 6. Pitch and Fundamental Frequency\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "        pitches = pitches[pitches > 0]\n",
    "        if len(pitches) > 0:\n",
    "            feature_vector['pitch_mean'] = np.mean(pitches)\n",
    "            feature_vector['pitch_std'] = np.std(pitches)\n",
    "        else:\n",
    "            feature_vector['pitch_mean'] = 0\n",
    "            feature_vector['pitch_std'] = 0\n",
    "        \n",
    "        # 7. Energy and RMS\n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        feature_vector['rms_mean'] = np.mean(rms)\n",
    "        feature_vector['rms_std'] = np.std(rms)\n",
    "        \n",
    "        # 8. Tempo\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        feature_vector['tempo'] = tempo\n",
    "        \n",
    "        # 9. Spectral Contrast\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        feature_vector['spectral_contrast_mean'] = np.mean(spectral_contrast)\n",
    "        feature_vector['spectral_contrast_std'] = np.std(spectral_contrast)\n",
    "        \n",
    "        # Add file information\n",
    "        feature_vector['file_path'] = audio_info['file_path']\n",
    "        feature_vector['duration'] = audio_info['duration']\n",
    "        \n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Example usage:\n",
    "features_df = extract_audio_features(audio_files)\n",
    "print(f\"Extracted {len(features_df.columns)} features from {len(features_df)} audio files\")\n",
    "'''\n",
    "# =============================================================================\n",
    "# 3. SIMULATED DATA FOR DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "# Since we don't have access to your actual audio files, let's create simulated data\n",
    "# Replace this section with your actual feature extraction when you have the data\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Simulate realistic audio features\n",
    "features_df = pd.DataFrame({\n",
    "    'spectral_centroid_mean': np.random.normal(2000, 500, n_samples),\n",
    "    'spectral_centroid_std': np.random.normal(300, 100, n_samples),\n",
    "    'spectral_rolloff_mean': np.random.normal(4000, 1000, n_samples),\n",
    "    'spectral_rolloff_std': np.random.normal(500, 150, n_samples),\n",
    "    'spectral_bandwidth_mean': np.random.normal(1500, 400, n_samples),\n",
    "    'spectral_bandwidth_std': np.random.normal(200, 50, n_samples),\n",
    "    'zcr_mean': np.random.normal(0.1, 0.03, n_samples),\n",
    "    'zcr_std': np.random.normal(0.02, 0.01, n_samples),\n",
    "    'rms_mean': np.random.normal(0.02, 0.01, n_samples),\n",
    "    'rms_std': np.random.normal(0.005, 0.002, n_samples),\n",
    "    'tempo': np.random.normal(120, 30, n_samples),\n",
    "    'pitch_mean': np.random.normal(150, 50, n_samples),\n",
    "    'pitch_std': np.random.normal(30, 10, n_samples),\n",
    "    'duration': np.random.uniform(1, 10, n_samples),\n",
    "})\n",
    "\n",
    "# Add MFCC features\n",
    "for i in range(13):\n",
    "    features_df[f'mfcc_{i}_mean'] = np.random.normal(0, 1, n_samples)\n",
    "    features_df[f'mfcc_{i}_std'] = np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "print(f\"Dataset shape: {features_df.shape}\")\n",
    "print(f\"Features: {list(features_df.columns)}\")\n",
    "'''\n",
    "# =============================================================================\n",
    "# 4. EXPLORATORY DATA ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def perform_eda(df):\n",
    "    \"\"\"Perform exploratory data analysis\"\"\"\n",
    "    print(\"=== EXPLORATORY DATA ANALYSIS ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n1. Basic Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\n2. Missing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Correlation matrix\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    correlation_matrix = df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Distribution of key features\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    key_features = ['spectral_centroid_mean', 'rms_mean', 'tempo', 'pitch_mean', 'zcr_mean', 'duration']\n",
    "    \n",
    "    for i, feature in enumerate(key_features):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        axes[row, col].hist(df[feature], bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[row, col].set_title(f'Distribution of {feature}')\n",
    "        axes[row, col].set_xlabel(feature)\n",
    "        axes[row, col].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "# Perform EDA\n",
    "correlation_matrix = perform_eda(features_df)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. DATA PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the data for clustering\"\"\"\n",
    "    print(\"=== DATA PREPROCESSING ===\")\n",
    "    \n",
    "    # Select numeric features only\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns\n",
    "    X = df[numeric_features].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Remove features with very low variance\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    variance_threshold = VarianceThreshold(threshold=0.01)\n",
    "    X_var = variance_threshold.fit_transform(X)\n",
    "    selected_features = X.columns[variance_threshold.get_support()]\n",
    "    X = X[selected_features]\n",
    "    \n",
    "    print(f\"Features after variance filtering: {X.shape[1]}\")\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    print(f\"Final dataset shape: {X_scaled_df.shape}\")\n",
    "    \n",
    "    return X_scaled_df, X, scaler\n",
    "\n",
    "# Preprocess data\n",
    "X_scaled_df, X_original, scaler = preprocess_data(features_df)\n",
    "\n",
    "# =============================================================================\n",
    "# 6. DIMENSIONALITY REDUCTION\n",
    "# =============================================================================\n",
    "\n",
    "def perform_dimensionality_reduction(X_scaled):\n",
    "    \"\"\"Perform dimensionality reduction for visualization and clustering\"\"\"\n",
    "    print(\"=== DIMENSIONALITY REDUCTION ===\")\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"PCA: Reduced from {X_scaled.shape[1]} to {X_pca.shape[1]} components\")\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_[:5]}\")\n",
    "    \n",
    "    # t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot PCA results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA Explained Variance')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title('PCA - First Two Components')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.6)\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    plt.title('t-SNE Visualization')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return X_pca, X_tsne, pca\n",
    "\n",
    "# Perform dimensionality reduction\n",
    "X_pca, X_tsne, pca = perform_dimensionality_reduction(X_scaled_df.values)\n",
    "\n",
    "# =============================================================================\n",
    "# 7. CLUSTERING ALGORITHMS\n",
    "# =============================================================================\n",
    "\n",
    "def find_optimal_clusters(X, max_clusters=10):\n",
    "    \"\"\"Find optimal number of clusters using multiple metrics\"\"\"\n",
    "    print(\"=== FINDING OPTIMAL NUMBER OF CLUSTERS ===\")\n",
    "    \n",
    "    metrics = {\n",
    "        'inertia': [],\n",
    "        'silhouette': [],\n",
    "        'calinski_harabasz': [],\n",
    "        'davies_bouldin': []\n",
    "    }\n",
    "    \n",
    "    K_range = range(2, max_clusters + 1)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        metrics['inertia'].append(kmeans.inertia_)\n",
    "        metrics['silhouette'].append(silhouette_score(X, cluster_labels))\n",
    "        metrics['calinski_harabasz'].append(calinski_harabasz_score(X, cluster_labels))\n",
    "        metrics['davies_bouldin'].append(davies_bouldin_score(X, cluster_labels))\n",
    "    \n",
    "    # Plot metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].plot(K_range, metrics['inertia'], 'bo-')\n",
    "    axes[0, 0].set_title('Elbow Method (Inertia)')\n",
    "    axes[0, 0].set_xlabel('Number of Clusters')\n",
    "    axes[0, 0].set_ylabel('Inertia')\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    axes[0, 1].plot(K_range, metrics['silhouette'], 'ro-')\n",
    "    axes[0, 1].set_title('Silhouette Score')\n",
    "    axes[0, 1].set_xlabel('Number of Clusters')\n",
    "    axes[0, 1].set_ylabel('Silhouette Score')\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    axes[1, 0].plot(K_range, metrics['calinski_harabasz'], 'go-')\n",
    "    axes[1, 0].set_title('Calinski-Harabasz Score')\n",
    "    axes[1, 0].set_xlabel('Number of Clusters')\n",
    "    axes[1, 0].set_ylabel('Calinski-Harabasz Score')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    axes[1, 1].plot(K_range, metrics['davies_bouldin'], 'mo-')\n",
    "    axes[1, 1].set_title('Davies-Bouldin Score')\n",
    "    axes[1, 1].set_xlabel('Number of Clusters')\n",
    "    axes[1, 1].set_ylabel('Davies-Bouldin Score')\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal k\n",
    "    optimal_k_silhouette = K_range[np.argmax(metrics['silhouette'])]\n",
    "    optimal_k_calinski = K_range[np.argmax(metrics['calinski_harabasz'])]\n",
    "    optimal_k_davies = K_range[np.argmin(metrics['davies_bouldin'])]\n",
    "    \n",
    "    print(f\"Optimal k (Silhouette): {optimal_k_silhouette}\")\n",
    "    print(f\"Optimal k (Calinski-Harabasz): {optimal_k_calinski}\")\n",
    "    print(f\"Optimal k (Davies-Bouldin): {optimal_k_davies}\")\n",
    "    \n",
    "    return optimal_k_silhouette, metrics\n",
    "\n",
    "# Find optimal clusters\n",
    "optimal_k, cluster_metrics = find_optimal_clusters(X_pca)\n",
    "\n",
    "# =============================================================================\n",
    "# 8. APPLY MULTIPLE CLUSTERING ALGORITHMS\n",
    "# =============================================================================\n",
    "\n",
    "def apply_clustering_algorithms(X, X_pca, optimal_k):\n",
    "    \"\"\"Apply multiple clustering algorithms\"\"\"\n",
    "    print(\"=== APPLYING CLUSTERING ALGORITHMS ===\")\n",
    "    \n",
    "    clustering_results = {}\n",
    "    \n",
    "    # 1. K-Means\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    clustering_results['kmeans'] = kmeans.fit_predict(X_pca)\n",
    "    \n",
    "    # 2. Agglomerative Clustering\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "    clustering_results['agglomerative'] = agg_clustering.fit_predict(X_pca)\n",
    "    \n",
    "    # 3. DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    clustering_results['dbscan'] = dbscan.fit_predict(X_pca)\n",
    "    \n",
    "    # Print clustering results\n",
    "    for method, labels in clustering_results.items():\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        print(f\"{method.upper()}: {n_clusters} clusters, {n_noise} noise points\")\n",
    "        \n",
    "        if n_clusters > 1:\n",
    "            silhouette_avg = silhouette_score(X_pca, labels)\n",
    "            print(f\"  Silhouette Score: {silhouette_avg:.3f}\")\n",
    "    \n",
    "    return clustering_results\n",
    "\n",
    "# Apply clustering\n",
    "clustering_results = apply_clustering_algorithms(X_scaled_df.values, X_pca, optimal_k)\n",
    "\n",
    "# =============================================================================\n",
    "# 9. VISUALIZATION OF CLUSTERS\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_clusters(X_tsne, clustering_results):\n",
    "    \"\"\"Visualize clustering results\"\"\"\n",
    "    print(\"=== VISUALIZING CLUSTERS ===\")\n",
    "    \n",
    "    n_methods = len(clustering_results)\n",
    "    fig, axes = plt.subplots(1, n_methods, figsize=(5 * n_methods, 5))\n",
    "    \n",
    "    if n_methods == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (method, labels) in enumerate(clustering_results.items()):\n",
    "        scatter = axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, \n",
    "                                 cmap='viridis', alpha=0.6, s=20)\n",
    "        axes[i].set_title(f'{method.upper()} Clustering')\n",
    "        axes[i].set_xlabel('t-SNE 1')\n",
    "        axes[i].set_ylabel('t-SNE 2')\n",
    "        plt.colorbar(scatter, ax=axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize clusters\n",
    "visualize_clusters(X_tsne, clustering_results)\n",
    "\n",
    "# =============================================================================\n",
    "# 10. CLUSTER ANALYSIS AND INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_clusters(X_original, clustering_results, feature_names):\n",
    "    \"\"\"Analyze and interpret clusters\"\"\"\n",
    "    print(\"=== CLUSTER ANALYSIS ===\")\n",
    "    \n",
    "    # Use K-means results for detailed analysis\n",
    "    labels = clustering_results['kmeans']\n",
    "    \n",
    "    # Add cluster labels to original features\n",
    "    analysis_df = X_original.copy()\n",
    "    analysis_df['cluster'] = labels\n",
    "    \n",
    "    # Cluster statistics\n",
    "    cluster_stats = analysis_df.groupby('cluster').agg(['mean', 'std'])\n",
    "    \n",
    "    print(\"Cluster Statistics (Mean values):\")\n",
    "    print(cluster_stats.xs('mean', level=1, axis=1))\n",
    "    \n",
    "    # Feature importance for each cluster\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    key_features = ['spectral_centroid_mean', 'rms_mean', 'tempo', 'pitch_mean', 'zcr_mean']\n",
    "    \n",
    "    for i, feature in enumerate(key_features):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        cluster_means = analysis_df.groupby('cluster')[feature].mean()\n",
    "        plt.bar(cluster_means.index, cluster_means.values)\n",
    "        plt.title(f'{feature} by Cluster')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel(f'Mean {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Emotional interpretation (hypothetical)\n",
    "    print(\"\\n=== EMOTIONAL INTERPRETATION ===\")\n",
    "    for cluster_id in sorted(labels.unique()):\n",
    "        cluster_data = analysis_df[analysis_df['cluster'] == cluster_id]\n",
    "        n_samples = len(cluster_data)\n",
    "        \n",
    "        # Analyze key characteristics\n",
    "        high_pitch = cluster_data['pitch_mean'].mean() > analysis_df['pitch_mean'].mean()\n",
    "        high_energy = cluster_data['rms_mean'].mean() > analysis_df['rms_mean'].mean()\n",
    "        high_tempo = cluster_data['tempo'].mean() > analysis_df['tempo'].mean()\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id} ({n_samples} samples):\")\n",
    "        print(f\"  High Pitch: {high_pitch}\")\n",
    "        print(f\"  High Energy: {high_energy}\")\n",
    "        print(f\"  High Tempo: {high_tempo}\")\n",
    "        \n",
    "        # Hypothetical emotion mapping\n",
    "        if high_pitch and high_energy and high_tempo:\n",
    "            emotion = \"Excitement/Joy\"\n",
    "        elif high_pitch and high_energy and not high_tempo:\n",
    "            emotion = \"Anger/Frustration\"\n",
    "        elif not high_pitch and high_energy:\n",
    "            emotion = \"Determination/Focus\"\n",
    "        elif not high_pitch and not high_energy:\n",
    "            emotion = \"Sadness/Calm\"\n",
    "        else:\n",
    "            emotion = \"Neutral/Mixed\"\n",
    "        \n",
    "        print(f\"  Likely Emotion: {emotion}\")\n",
    "    \n",
    "    return analysis_df\n",
    "\n",
    "# Analyze clusters\n",
    "analysis_df = analyze_clusters(X_original, clustering_results, X_original.columns)\n",
    "\n",
    "# =============================================================================\n",
    "# 11. MODEL EVALUATION AND COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_clustering_models(X_pca, clustering_results):\n",
    "    \"\"\"Evaluate different clustering models\"\"\"\n",
    "    print(\"=== MODEL EVALUATION ===\")\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for method, labels in clustering_results.items():\n",
    "        if len(set(labels)) > 1:  # Skip if only one cluster\n",
    "            try:\n",
    "                silhouette_avg = silhouette_score(X_pca, labels)\n",
    "                calinski_harabasz = calinski_harabasz_score(X_pca, labels)\n",
    "                davies_bouldin = davies_bouldin_score(X_pca, labels)\n",
    "                \n",
    "                evaluation_results[method] = {\n",
    "                    'silhouette_score': silhouette_avg,\n",
    "                    'calinski_harabasz_score': calinski_harabasz,\n",
    "                    'davies_bouldin_score': davies_bouldin,\n",
    "                    'n_clusters': len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                }\n",
    "            except:\n",
    "                print(f\"Could not evaluate {method}\")\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    eval_df = pd.DataFrame(evaluation_results).T\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(eval_df)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    eval_df['silhouette_score'].plot(kind='bar', ax=axes[0], title='Silhouette Score')\n",
    "    eval_df['calinski_harabasz_score'].plot(kind='bar', ax=axes[1], title='Calinski-Harabasz Score')\n",
    "    eval_df['davies_bouldin_score'].plot(kind='bar', ax=axes[2], title='Davies-Bouldin Score')\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "# Evaluate models\n",
    "evaluation_df = evaluate_clustering_models(X_pca, clustering_results)\n",
    "\n",
    "# =============================================================================\n",
    "# 12. HYPERPARAMETER TUNING\n",
    "# =============================================================================\n",
    "\n",
    "def hyperparameter_tuning(X_pca):\n",
    "    \"\"\"Perform hyperparameter tuning for clustering algorithms\"\"\"\n",
    "    print(\"=== HYPERPARAMETER TUNING ===\")\n",
    "    \n",
    "    # K-means tuning\n",
    "    best_kmeans_score = -1\n",
    "    best_kmeans_params = {}\n",
    "    \n",
    "    for n_clusters in range(2, 11):\n",
    "        for init in ['k-means++', 'random']:\n",
    "            kmeans = KMeans(n_clusters=n_clusters, init=init, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(X_pca)\n",
    "            score = silhouette_score(X_pca, labels)\n",
    "            \n",
    "            if score > best_kmeans_score:\n",
    "                best_kmeans_score = score\n",
    "                best_kmeans_params = {'n_clusters': n_clusters, 'init': init}\n",
    "    \n",
    "    print(f\"Best K-means parameters: {best_kmeans_params}\")\n",
    "    print(f\"Best K-means silhouette score: {best_kmeans_score:.3f}\")\n",
    "    \n",
    "    # DBSCAN tuning\n",
    "    best_dbscan_score = -1\n",
    "    best_dbscan_params = {}\n",
    "    \n",
    "    for eps in [0.3, 0.5, 0.7, 1.0]:\n",
    "        for min_samples in [3, 5, 7, 10]:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(X_pca)\n",
    "            \n",
    "            if len(set(labels)) > 1:\n",
    "                score = silhouette_score(X_pca, labels)\n",
    "                \n",
    "                if score > best_dbscan_score:\n",
    "                    best_dbscan_score = score\n",
    "                    best_dbscan_params = {'eps': eps, 'min_samples': min_samples}\n",
    "    \n",
    "    print(f\"Best DBSCAN parameters: {best_dbscan_params}\")\n",
    "    print(f\"Best DBSCAN silhouette score: {best_dbscan_score:.3f}\")\n",
    "    \n",
    "    return best_kmeans_params, best_dbscan_params\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "best_kmeans_params, best_dbscan_params = hyperparameter_tuning(X_pca)\n",
    "\n",
    "# =============================================================================\n",
    "# 13. FINAL MODEL AND RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def build_final_model(X_pca, best_params):\n",
    "    \"\"\"Build final model with best parameters\"\"\"\n",
    "    print(\"=== BUILDING FINAL MODEL ===\")\n",
    "    \n",
    "    # Build final K-means model\n",
    "    final_kmeans = KMeans(**best_params, random_state=42, n_init=10)\n",
    "    final_labels = final_kmeans.fit_predict(X_pca)\n",
    "    \n",
    "    # Model performance\n",
    "    final_silhouette = silhouette_score(X_pca, final_labels)\n",
    "    final_calinski = calinski_harabasz_score(X_pca, final_labels)\n",
    "    final_davies = davies_bouldin_score(X_pca, final_labels)\n",
    "    \n",
    "    print(f\"Final Model Performance:\")\n",
    "    print(f\"  Silhouette Score: {final_silhouette:.3f}\")\n",
    "    print(f\"  Calinski-Harabasz Score: {final_calinski:.3f}\")\n",
    "    print(f\"  Davies-Bouldin Score: {final_davies:.3f}\")\n",
    "    print(f\"  Number of Clusters: {len(set(final_labels))}\")\n",
    "    \n",
    "    return final_kmeans, final_labels\n",
    "\n",
    "# Build final model\n",
    "final_model, final_labels = build_final_model(X_pca, best_kmeans_params)\n",
    "\n",
    "# =============================================================================\n",
    "# 14. EXPORT RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "def export_results(features_df, final_labels, output_path='emotion_analysis_results.csv'):\n",
    "    \"\"\"Export results to CSV\"\"\"\n",
    "    print(\"=== EXPORTING RESULTS ===\")\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = features_df.copy()\n",
    "    results_df['predicted_emotion_cluster'] = final_labels\n",
    "    \n",
    "    # Map clusters to emotion labels (hypothetical)\n",
    "    emotion_mapping = {\n",
    "        0: 'Neutral',\n",
    "        1: 'Happy/Excited',\n",
    "        2: 'Sad/Calm',\n",
    "        3: 'Angry/Frustrated',\n",
    "        4: 'Surprised/Energetic'\n",
    "    }\n",
    "    \n",
    "    results_df['predicted_emotion'] = results_df['predicted_emotion_cluster'].map(\n",
    "        lambda x: emotion_mapping.get(x, f'Cluster_{x}')\n",
    "    )\n",
    "    \n",
    "    # Export to CSV\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results exported to {output_path}\")\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\nSample Results:\")\n",
    "    print(results_df[['spectral_centroid_mean', 'rms_mean', 'tempo', 'pitch_mean', \n",
    "                     'predicted_emotion_cluster', 'predicted_emotion']].head(10))\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Export results\n",
    "final_results = export_results(features_df, final_labels)\n",
    "\n",
    "# =============================================================================\n",
    "# 15. SUMMARY AND RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 15. SUMMARY AND RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== SUMMARY AND RECOMMENDATIONS ===\")\n",
    "print(f\"\"\"\n",
    "Project Summary:\n",
    "- Processed {len(features_df)} audio samples\n",
    "- Extracted {len(X_original.columns)} audio features\n",
    "- Applied PCA for dimensionality reduction ({X_pca.shape[1]} components)\n",
    "- Tested multiple clustering algorithms (K-means, Agglomerative, DBSCAN)\n",
    "- Achieved best silhouette score of {evaluation_df['silhouette_score'].max():.3f}\n",
    "- Identified {len(set(final_labels))} emotion clusters\n",
    "\n",
    "Key Findings:\n",
    "1. Spectral features (centroid, rolloff, bandwidth) are important for emotion distinction\n",
    "2. Energy (RMS) and pitch features correlate with emotional intensity\n",
    "3. MFCCs capture timbral characteristics useful for emotion clustering\n",
    "4. Optimal number of clusters appears to be around {best_kmeans_params['n_clusters']}\n",
    "5. K-means performed best among tested algorithms\n",
    "\n",
    "Recommendations for Real Implementation:\n",
    "1. Use actual audio files with the load_audio_files() and extract_audio_features() functions\n",
    "2. Consider additional features like prosodic features, formants, and jitter/shimmer\n",
    "3. Experiment with ensemble clustering methods\n",
    "4. Validate clusters with domain expert knowledge\n",
    "5. Consider semi-supervised approaches if some labels are available\n",
    "\n",
    "Next Steps:\n",
    "1. Collect more diverse audio samples\n",
    "2. Implement deep learning approaches (autoencoders, CNNs)\n",
    "3. Add temporal modeling for sequential audio analysis\n",
    "4. Develop real-time emotion detection system\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# 16. ADVANCED TECHNIQUES (BONUS)\n",
    "# =============================================================================\n",
    "\n",
    "def advanced_clustering_techniques(X_pca, X_scaled):\n",
    "    \"\"\"Apply advanced clustering techniques\"\"\"\n",
    "    print(\"=== ADVANCED CLUSTERING TECHNIQUES ===\")\n",
    "    \n",
    "    # 1. Gaussian Mixture Models\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=optimal_k, random_state=42)\n",
    "    gmm_labels = gmm.fit_predict(X_pca)\n",
    "    gmm_score = silhouette_score(X_pca, gmm_labels)\n",
    "    \n",
    "    print(f\"Gaussian Mixture Model - Silhouette Score: {gmm_score:.3f}\")\n",
    "    \n",
    "    # 2. Spectral Clustering\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    \n",
    "    spectral = SpectralClustering(n_clusters=optimal_k, random_state=42)\n",
    "    spectral_labels = spectral.fit_predict(X_pca)\n",
    "    spectral_score = silhouette_score(X_pca, spectral_labels)\n",
    "    \n",
    "    print(f\"Spectral Clustering - Silhouette Score: {spectral_score:.3f}\")\n",
    "    \n",
    "    # 3. Mini-Batch K-Means (for large datasets)\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    \n",
    "    mini_kmeans = MiniBatchKMeans(n_clusters=optimal_k, random_state=42)\n",
    "    mini_labels = mini_kmeans.fit_predict(X_pca)\n",
    "    mini_score = silhouette_score(X_pca, mini_labels)\n",
    "    \n",
    "    print(f\"Mini-Batch K-Means - Silhouette Score: {mini_score:.3f}\")\n",
    "    \n",
    "    # Visualize advanced clustering results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    scatter1 = axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=gmm_labels, cmap='viridis', alpha=0.6)\n",
    "    axes[0].set_title('Gaussian Mixture Model')\n",
    "    axes[0].set_xlabel('t-SNE 1')\n",
    "    axes[0].set_ylabel('t-SNE 2')\n",
    "    plt.colorbar(scatter1, ax=axes[0])\n",
    "    \n",
    "    scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=spectral_labels, cmap='viridis', alpha=0.6)\n",
    "    axes[1].set_title('Spectral Clustering')\n",
    "    axes[1].set_xlabel('t-SNE 1')\n",
    "    axes[1].set_ylabel('t-SNE 2')\n",
    "    plt.colorbar(scatter2, ax=axes[1])\n",
    "    \n",
    "    scatter3 = axes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=mini_labels, cmap='viridis', alpha=0.6)\n",
    "    axes[2].set_title('Mini-Batch K-Means')\n",
    "    axes[2].set_xlabel('t-SNE 1')\n",
    "    axes[2].set_ylabel('t-SNE 2')\n",
    "    plt.colorbar(scatter3, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'gmm': gmm_labels,\n",
    "        'spectral': spectral_labels,\n",
    "        'mini_kmeans': mini_labels\n",
    "    }\n",
    "\n",
    "# Apply advanced clustering techniques\n",
    "advanced_results = advanced_clustering_techniques(X_pca, X_scaled_df.values)\n",
    "\n",
    "# =============================================================================\n",
    "# 17. ENSEMBLE CLUSTERING\n",
    "# =============================================================================\n",
    "\n",
    "def ensemble_clustering(clustering_results, advanced_results):\n",
    "    \"\"\"Create ensemble clustering from multiple algorithms\"\"\"\n",
    "    print(\"=== ENSEMBLE CLUSTERING ===\")\n",
    "    \n",
    "    # Combine all clustering results\n",
    "    all_results = {**clustering_results, **advanced_results}\n",
    "    \n",
    "    # Create consensus clustering using voting\n",
    "    n_samples = len(list(all_results.values())[0])\n",
    "    consensus_matrix = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    for method, labels in all_results.items():\n",
    "        if len(set(labels)) > 1:  # Skip single cluster results\n",
    "            for i in range(n_samples):\n",
    "                for j in range(n_samples):\n",
    "                    if labels[i] == labels[j]:\n",
    "                        consensus_matrix[i, j] += 1\n",
    "    \n",
    "    # Normalize consensus matrix\n",
    "    consensus_matrix = consensus_matrix / len(all_results)\n",
    "    \n",
    "    # Apply clustering to consensus matrix\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    \n",
    "    ensemble_clustering = AgglomerativeClustering(\n",
    "        n_clusters=optimal_k, \n",
    "        linkage='average',\n",
    "        affinity='precomputed'\n",
    "    )\n",
    "    \n",
    "    # Convert consensus matrix to distance matrix\n",
    "    distance_matrix = 1 - consensus_matrix\n",
    "    ensemble_labels = ensemble_clustering.fit_predict(distance_matrix)\n",
    "    \n",
    "    # Evaluate ensemble clustering\n",
    "    ensemble_score = silhouette_score(X_pca, ensemble_labels)\n",
    "    print(f\"Ensemble Clustering - Silhouette Score: {ensemble_score:.3f}\")\n",
    "    \n",
    "    # Visualize ensemble results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(consensus_matrix, cmap='viridis', aspect='auto')\n",
    "    plt.title('Consensus Matrix')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Sample Index')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=ensemble_labels, cmap='viridis', alpha=0.6)\n",
    "    plt.title('Ensemble Clustering')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    plt.colorbar(scatter)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ensemble_labels, consensus_matrix\n",
    "\n",
    "# Create ensemble clustering\n",
    "ensemble_labels, consensus_matrix = ensemble_clustering(clustering_results, advanced_results)\n",
    "\n",
    "# =============================================================================\n",
    "# 18. CLUSTER VALIDATION AND STABILITY\n",
    "# =============================================================================\n",
    "\n",
    "def cluster_stability_analysis(X_pca, n_iterations=10):\n",
    "    \"\"\"Analyze cluster stability across multiple runs\"\"\"\n",
    "    print(\"=== CLUSTER STABILITY ANALYSIS ===\")\n",
    "    \n",
    "    stability_scores = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Add small random noise to test stability\n",
    "        X_noisy = X_pca + np.random.normal(0, 0.01, X_pca.shape)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=i, n_init=10)\n",
    "        labels = kmeans.fit_predict(X_noisy)\n",
    "        \n",
    "        silhouette_avg = silhouette_score(X_noisy, labels)\n",
    "        stability_scores.append(silhouette_avg)\n",
    "    \n",
    "    mean_stability = np.mean(stability_scores)\n",
    "    std_stability = np.std(stability_scores)\n",
    "    \n",
    "    print(f\"Cluster Stability:\")\n",
    "    print(f\"  Mean Silhouette Score: {mean_stability:.3f}\")\n",
    "    print(f\"  Standard Deviation: {std_stability:.3f}\")\n",
    "    print(f\"  Stability Rating: {'High' if std_stability < 0.05 else 'Medium' if std_stability < 0.1 else 'Low'}\")\n",
    "    \n",
    "    # Plot stability\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, n_iterations + 1), stability_scores, 'bo-')\n",
    "    plt.axhline(y=mean_stability, color='r', linestyle='--', label=f'Mean: {mean_stability:.3f}')\n",
    "    plt.fill_between(range(1, n_iterations + 1), \n",
    "                     mean_stability - std_stability, \n",
    "                     mean_stability + std_stability, \n",
    "                     alpha=0.2, color='red')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Cluster Stability Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return stability_scores\n",
    "\n",
    "# Analyze cluster stability\n",
    "stability_scores = cluster_stability_analysis(X_pca)\n",
    "\n",
    "# =============================================================================\n",
    "# 19. FEATURE IMPORTANCE AND INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_importance(X_original, final_labels):\n",
    "    \"\"\"Analyze feature importance for clustering\"\"\"\n",
    "    print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "    \n",
    "    # Calculate feature importance using Random Forest\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    # Encode labels for classification\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(final_labels)\n",
    "    \n",
    "    # Train Random Forest to understand feature importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_original, y_encoded)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_original.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 15 Most Important Features:\")\n",
    "    print(feature_importance.head(15))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Feature Importance for Emotion Clustering')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(X_original, final_labels)\n",
    "\n",
    "# =============================================================================\n",
    "# 20. REAL-TIME EMOTION DETECTION FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "def create_emotion_detection_pipeline():\n",
    "    \"\"\"Create a pipeline for real-time emotion detection\"\"\"\n",
    "    print(\"=== REAL-TIME EMOTION DETECTION PIPELINE ===\")\n",
    "    \n",
    "    class EmotionDetector:\n",
    "        def __init__(self, model, scaler, pca_model):\n",
    "            self.model = model\n",
    "            self.scaler = scaler\n",
    "            self.pca_model = pca_model\n",
    "            self.emotion_mapping = {\n",
    "                0: 'Neutral',\n",
    "                1: 'Happy/Excited',\n",
    "                2: 'Sad/Calm',\n",
    "                3: 'Angry/Frustrated',\n",
    "                4: 'Surprised/Energetic'\n",
    "            }\n",
    "        \n",
    "        def extract_features_single(self, audio_file):\n",
    "            \"\"\"Extract features from a single audio file\"\"\"\n",
    "            # This would be implemented with actual audio processing\n",
    "            # For now, return dummy features\n",
    "            return np.random.normal(0, 1, len(X_original.columns))\n",
    "        \n",
    "        def predict_emotion(self, audio_file):\n",
    "            \"\"\"Predict emotion from audio file\"\"\"\n",
    "            # Extract features\n",
    "            features = self.extract_features_single(audio_file)\n",
    "            \n",
    "            # Scale features\n",
    "            features_scaled = self.scaler.transform(features.reshape(1, -1))\n",
    "            \n",
    "            # Apply PCA\n",
    "            features_pca = self.pca_model.transform(features_scaled)\n",
    "            \n",
    "            # Predict cluster\n",
    "            cluster = self.model.predict(features_pca)[0]\n",
    "            \n",
    "            # Map to emotion\n",
    "            emotion = self.emotion_mapping.get(cluster, f'Cluster_{cluster}')\n",
    "            \n",
    "            return emotion, cluster\n",
    "        \n",
    "        def predict_batch(self, audio_files):\n",
    "            \"\"\"Predict emotions for multiple files\"\"\"\n",
    "            results = []\n",
    "            for audio_file in audio_files:\n",
    "                emotion, cluster = self.predict_emotion(audio_file)\n",
    "                results.append({\n",
    "                    'file': audio_file,\n",
    "                    'emotion': emotion,\n",
    "                    'cluster': cluster\n",
    "                })\n",
    "            return results\n",
    "    \n",
    "    # Create emotion detector instance\n",
    "    detector = EmotionDetector(final_model, scaler, pca)\n",
    "    \n",
    "    print(\"Emotion Detection Pipeline Created!\")\n",
    "    print(\"Usage:\")\n",
    "    print(\"  detector.predict_emotion('path/to/audio.wav')\")\n",
    "    print(\"  detector.predict_batch(['audio1.wav', 'audio2.wav'])\")\n",
    "    \n",
    "    return detector\n",
    "\n",
    "# Create emotion detection pipeline\n",
    "emotion_detector = create_emotion_detection_pipeline()\n",
    "\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
